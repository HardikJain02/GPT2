{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Base Implementation with Group Query Attention**"
      ],
      "metadata": {
        "id": "ZGm4lAmEYsQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules import ModuleList\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "Ilg1OPCr80I5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mQjr0xSH83xt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, epsilon=1e-12):\n",
        "      \"\"\"\n",
        "      Initialize LayerNorm module.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "\n",
        "      # Learnable weight parameter for scaling.\n",
        "      self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "\n",
        "      # Learnable bias parameter for shifting.\n",
        "      self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "      # Small value to avoid division by zero in normalization.\n",
        "      self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      # Compute mean and variance along the last dimension.\n",
        "      u = x.mean(-1, keepdim=True)\n",
        "      s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "\n",
        "      # Normalize the input tensor.\n",
        "      x = (x - u) / torch.sqrt(s + self.epsilon)\n",
        "\n",
        "      # Scale and shift using learnable parameters.\n",
        "      return self.weight * x + self.bias"
      ],
      "metadata": {
        "id": "B7jXEEUr87Ly"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nx, nf):\n",
        "        '''\n",
        "        The CONV 1D layer can be thought of as a linear layer itself.\n",
        "        It is casting an initial tensor x (having the final\n",
        "        dimension of x.size(-1)) being passed to it to have a final dimension\n",
        "        of size self.nf.\n",
        "\n",
        "        We do this to be able to cast the input to query, key and value matrices.\n",
        "\n",
        "        nx: Number of input features.\n",
        "        nf: Number of filters (output channels).\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        #intialising an empty matrix as weights for size of (nx)X(nf)\n",
        "        w = torch.empty(nx, nf)\n",
        "        #initialising these weights as normal distribution\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        #calculating the weights and biases by encoding them using nn.Parameter\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''x:The input tensor'''\n",
        "        #this size output is summation of x second dimension and the nf dimension\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        # dot multiplying Q,K(transpose) and V\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        #x.view helps in taking the transpose out\n",
        "        x = x.view(*size_out)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2fmuuJUl89j0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
        "        super().__init__()\n",
        "        self.c_fc    = Conv1D(d_model, nx)\n",
        "        self.c_proj  = Conv1D(nx, d_model)\n",
        "        self.act     = F.gelu\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
      ],
      "metadata": {
        "id": "QD_IXk1z8_ps"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s6RfU6jd8ayU"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=True, num_groups=2):\n",
        "        \"\"\"\n",
        "        Initialize Grouped Query Attention Layer module.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of attention heads.\n",
        "        self.n_head = n_head\n",
        "\n",
        "        # Dimensionality of the model.\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Number of query groups\n",
        "        self.num_groups = num_groups\n",
        "\n",
        "        # Check if the number of groups divides the dimensionality evenly\n",
        "        assert d_model % num_groups == 0, \"Number of groups must evenly divide the dimensionality.\"\n",
        "\n",
        "        # Dimensionality of each group\n",
        "        self.group_dim = d_model // num_groups\n",
        "\n",
        "        # Dimensionality of each head\n",
        "        self.head_dim = d_model // n_head\n",
        "\n",
        "        # 1D Convolutional Layer for attention weights computation.\n",
        "        self.c_attn = Conv1D(d_model, d_model * 3)\n",
        "\n",
        "        # Flag to scale attention scores.\n",
        "        self.scale = scale\n",
        "\n",
        "        # Softmax activation for attention scores.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Lower triangular bias matrix for masking future tokens.\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "\n",
        "        # Dropout layer for regularization.\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # 1D Convolutional Layer for output projection.\n",
        "        self.c_proj = Conv1D(d_model, d_model)\n",
        "\n",
        "        # Rotary Position Embedding\n",
        "        # self.rpe = RotaryPositionalEmbedding(self.head_dim, n_ctx)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Split the last dimension of the input tensor into multiple heads.\n",
        "        \"\"\"\n",
        "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def _attn(self, q, k, v, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Compute attention scores and apply attention to values.\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "        if self.scale:\n",
        "            scores = scores / torch.sqrt(torch.tensor(self.group_dim).float())  # Scale by sqrt(group_dim)\n",
        "        nd, ns = scores.size(-2), scores.size(-1)\n",
        "        if attn_mask is not None:\n",
        "            scores = scores + attn_mask\n",
        "        scores = self.softmax(scores)\n",
        "        scores = self.dropout(scores)\n",
        "        outputs = torch.matmul(scores, v)\n",
        "        return outputs\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        \"\"\"\n",
        "        Merge the heads back to the original shape.\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_shape)\n",
        "\n",
        "    def group_queries(self, x):\n",
        "        \"\"\"\n",
        "        Group queries based on the number of query groups.\n",
        "        \"\"\"\n",
        "        return torch.split(x, self.group_dim, dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Compute attention weights using 1D convolution.\n",
        "        x = self.c_attn(x)\n",
        "\n",
        "        # Split the tensor into query, key, and value.\n",
        "        q, k, v = x.split(self.d_model, dim=2)\n",
        "\n",
        "        # Split heads for query, key, and value.\n",
        "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "\n",
        "        # Apply RopE embeddings\n",
        "        # q = self.rpe(q)\n",
        "        # k = self.rpe(k)\n",
        "\n",
        "        # Group queries\n",
        "        grouped_queries = self.group_queries(q)\n",
        "\n",
        "        # Apply grouped attention mechanism\n",
        "        grouped_outputs = []\n",
        "        for group_queries in grouped_queries:\n",
        "            out = self._attn(group_queries, k, v)\n",
        "            grouped_outputs.append(out)\n",
        "\n",
        "        # Merge the grouped outputs\n",
        "        out = torch.cat(grouped_outputs, dim=-1)\n",
        "\n",
        "        # Merge the heads back to the original shape.\n",
        "        out = self.merge_heads(out)\n",
        "\n",
        "        # Apply output projection.\n",
        "        out = self.c_proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock_GroupQueryAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head, num_groups, n_ctx, dropout=0.1):\n",
        "      \"\"\"\n",
        "      Initialize Transformer Block module.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "\n",
        "      # GroupQuery-Attention Layer\n",
        "      self.attn = GroupedQueryAttention(d_model=d_model, n_head=n_head, d_head=64,\n",
        "                                        n_ctx=n_ctx, bias=True, scale=True,\n",
        "                                        num_groups=num_groups)\n",
        "\n",
        "      # Feedforward Layer\n",
        "      self.feedforward = FeedForward(dropout=0.1, d_model=d_model,\n",
        "                                     nx=d_model * 4)\n",
        "\n",
        "      # Layer Normalization for the attention output\n",
        "      self.ln_1 = LayerNorm(d_model)\n",
        "\n",
        "      # Layer Normalization for the feedforward output\n",
        "      self.ln_2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      # Self-Attention Layer with Layer Normalization and skip connection\n",
        "      x = x + self.attn(self.ln_1(x))\n",
        "\n",
        "      # Feedforward Layer with Layer Normalization and skip connection\n",
        "      x = x + self.feedforward(self.ln_2(x))\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "X8eVQBB08j0m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2_GroupQueryAttention(nn.Module):\n",
        "    def __init__(self,  n_layer=12 , n_ctx=1024, d_model=768, vcb_sz=50257,n_head=12,num_groups=2):\n",
        "        \"\"\"\n",
        "        Initialize GPT-2 model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of transformer layers\n",
        "        self.nlayers = n_layer\n",
        "\n",
        "        # Transformer block as the basic building unit\n",
        "        self.block = TransformerBlock_GroupQueryAttention(d_model=d_model, n_head=n_head,\n",
        "                                      n_ctx = n_ctx,\n",
        "                                      num_groups = num_groups,\n",
        "                                      dropout=0.1)\n",
        "\n",
        "        # List of transformer blocks forming the layers\n",
        "        self.h = nn.ModuleList([copy.deepcopy(self.block) for i in range(self.nlayers)])\n",
        "\n",
        "        # Word Embedding layer\n",
        "        self.wte = nn.Embedding(vcb_sz, d_model)\n",
        "\n",
        "\n",
        "        # Layer Normalization for the final output\n",
        "        self.ln_f = LayerNorm(d_model)\n",
        "\n",
        "        # Linear layer for output predictions\n",
        "        self.out = nn.Linear(d_model, vcb_sz, bias=False)\n",
        "\n",
        "        # CrossEntropyLoss for training\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights of the model.\n",
        "        \"\"\"\n",
        "        # Share weights between output layer and word embedding\n",
        "        self.out.weight = self.wte.weight\n",
        "\n",
        "        # Apply custom weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"\n",
        "        Custom weight initialization for linear, embedding, and convolutional layers.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, src, labels=None, pos_ids=None):\n",
        "\n",
        "        # Apply word embeddings\n",
        "        inp = self.wte(src)\n",
        "\n",
        "        # Forward pass through transformer layers\n",
        "        for i in range(self.nlayers):\n",
        "            inp = self.h[i](inp)\n",
        "\n",
        "        # Apply layer normalization to the final output\n",
        "        inp = self.ln_f(inp)\n",
        "\n",
        "        # Linear layer for output predictions\n",
        "        logits = self.out(inp)\n",
        "\n",
        "        # Prepare outputs\n",
        "        outputs = (logits,) + (inp,)\n",
        "\n",
        "        # If labels are provided, compute and return the loss\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "            return outputs\n",
        "\n",
        "        # Otherwise, return logits\n",
        "        return logits"
      ],
      "metadata": {
        "id": "HNroC79j8lw4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained_weights from hugging face\n",
        "# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.`\n",
        "\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin ."
      ],
      "metadata": {
        "id": "aHRxC_OE_BYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7c1858-b505-4593-f880-343072eddce7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-17 15:33:46--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.102.22, 52.217.126.64, 54.231.224.24, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.102.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548118077 (523M) [application/octet-stream]\n",
            "Saving to: ‘gpt2-pytorch_model.bin.1’\n",
            "\n",
            "gpt2-pytorch_model. 100%[===================>] 522.73M  73.0MB/s    in 8.1s    \n",
            "\n",
            "2023-12-17 15:33:54 (64.8 MB/s) - ‘gpt2-pytorch_model.bin.1’ saved [548118077/548118077]\n",
            "\n",
            "--2023-12-17 15:33:54--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2023-12-17 15:33:54--\n",
            "Total wall clock time: 8.2s\n",
            "Downloaded: 1 files, 523M in 8.1s (64.8 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2_GroupQueryAttention()\n",
        "\n",
        "model_dict = model.state_dict() #currently with random initialization\n",
        "state_dict = torch.load(\"/content/gpt2-pytorch_model.bin\") #pretrained weights\n",
        "\n",
        "old_keys = []\n",
        "new_keys = []\n",
        "for key in state_dict.keys():\n",
        "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
        "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
        "        new_keys.append(new_key)\n",
        "        old_keys.append(key)\n",
        "\n",
        "for old_key, new_key in zip(old_keys, new_keys):\n",
        "    state_dict[new_key]=state_dict.pop(old_key)\n",
        "\n",
        "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval() #model in inference mode as it's now initialized with pretrained weights"
      ],
      "metadata": {
        "id": "KZlZ_bvu8nsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7751749-816d-4b36-fdb1-246a0f58e815"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2_GroupQueryAttention(\n",
              "  (block): TransformerBlock_GroupQueryAttention(\n",
              "    (attn): GroupedQueryAttention(\n",
              "      (c_attn): Conv1D()\n",
              "      (softmax): Softmax(dim=-1)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (c_proj): Conv1D()\n",
              "    )\n",
              "    (feedforward): FeedForward(\n",
              "      (c_fc): Conv1D()\n",
              "      (c_proj): Conv1D()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (ln_1): LayerNorm()\n",
              "    (ln_2): LayerNorm()\n",
              "  )\n",
              "  (h): ModuleList(\n",
              "    (0-11): 12 x TransformerBlock_GroupQueryAttention(\n",
              "      (attn): GroupedQueryAttention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm()\n",
              "      (ln_2): LayerNorm()\n",
              "    )\n",
              "  )\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (ln_f): LayerNorm()\n",
              "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  (loss_fn): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "context = torch.tensor([tokenizer.encode(\"Hi Contlo, How\")])\n",
        "\n",
        "def generate(context, ntok=20):\n",
        "    start_time = time.time()\n",
        "    for _ in range(ntok):\n",
        "        out = model(context)\n",
        "        logits = out[:, -1, :]\n",
        "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = np.NINF\n",
        "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
        "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    return context, inference_time\n",
        "\n",
        "out, inference_time = generate(context, ntok=20)\n",
        "decoded_output = tokenizer.decode(out[0])\n",
        "\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "print(f\"Generated Output: {decoded_output}\")\n"
      ],
      "metadata": {
        "id": "mk2dfr4Q8qVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836bf502-0f17-4582-9fff-dbb5ed8ac655"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time: 2.5750 seconds\n",
            "Generated Output: Hi Contlo, How, or all or any any all or any of any any of any any of any any any any\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVJo88tM_GuI"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}