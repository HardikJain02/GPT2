{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Sliding Window Attention in the base GPT2 Implementation**"
      ],
      "metadata": {
        "id": "GD_3GYHmYZuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alstq9vJCgkz",
        "outputId": "d82a9712-9784-46b7-d4dd-480ef6097bd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m657.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Collecting torch==2.1.2 (from xformers)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->xformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.7.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 xformers-0.0.23.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules import ModuleList\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from typing import Optional\n",
        "from xformers.components.attention import (\n",
        "    AttentionMask,\n",
        "    maybe_sparsify,\n",
        "    sparsify\n",
        "\n",
        "\n",
        ")\n",
        "from xformers.components.attention.attention_patterns import (\n",
        "    causal_1d_pattern,\n",
        "    local_1d_pattern,\n",
        ")\n",
        "\n",
        "from torch import broadcast_tensors\n",
        "from einops import rearrange, repeat\n",
        "from einops import rearrange, repeat, pack, unpack\n",
        "from torch import nn, einsum\n",
        "\n"
      ],
      "metadata": {
        "id": "x8HZnLEIA7gj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TZ_k4Z_bA-Hj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, epsilon=1e-12):\n",
        "      \"\"\"\n",
        "      Initialize LayerNorm module.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "\n",
        "      # Learnable weight parameter for scaling.\n",
        "      self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "\n",
        "      # Learnable bias parameter for shifting.\n",
        "      self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "      # Small value to avoid division by zero in normalization.\n",
        "      self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      # Compute mean and variance along the last dimension.\n",
        "      u = x.mean(-1, keepdim=True)\n",
        "      s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "\n",
        "      # Normalize the input tensor.\n",
        "      x = (x - u) / torch.sqrt(s + self.epsilon)\n",
        "\n",
        "      # Scale and shift using learnable parameters.\n",
        "      return self.weight * x + self.bias"
      ],
      "metadata": {
        "id": "Ha7hSSpkBlor"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nx, nf):\n",
        "        '''\n",
        "        nx: Number of input features.\n",
        "        nf: Number of filters (output channels).\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        #intialising an empty matrix as weights for size of (nx)X(nf)\n",
        "        w = torch.empty(nx, nf)\n",
        "        #initialising these weights as normal distribution\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        #calculating the weights and biases by encoding them using nn.Parameter\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''x:The input tensor'''\n",
        "        #this size output is summation of x second dimension and the nf dimension\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        # dot multiplying Q,K(transpose) and V\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)#x.view helps in taking the transpose out\n",
        "        x = x.view(*size_out)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nQgfPDRoBoAA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
        "        super().__init__()\n",
        "        self.c_fc    = Conv1D(d_model, nx)\n",
        "        self.c_proj  = Conv1D(nx, d_model)\n",
        "        self.act     = F.gelu\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
      ],
      "metadata": {
        "id": "saCYTFKNBsR5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changing Mulihead Attention in the base implementation with Sliding Window Attention**"
      ],
      "metadata": {
        "id": "jOXp0pcxB8ZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IbbPwrtW_t1J"
      },
      "outputs": [],
      "source": [
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(value, d):\n",
        "    return d if not exists(value) else value\n",
        "\n",
        "def to(t):\n",
        "    return {'device': t.device, 'dtype': t.dtype}\n",
        "\n",
        "def max_neg_value(tensor):\n",
        "    return -torch.finfo(tensor.dtype).max\n",
        "\n",
        "def pad_to_multiple(tensor, multiple, dim=-1, value=0):\n",
        "    '''Function for padding over tensor for how many times which is multiple here\n",
        "    if the seqlen is not a muliple of multiple so we need to pad the remaining for which we caluclate the remainder and then pad the remainder\n",
        "    Params:\n",
        "    tensor:The tensor that needs to be pad\n",
        "    multiple:The multiple upto which padding is happenining\n",
        "    dim: the dimension accross for padding\n",
        "    value:what should the padded values'''\n",
        "    seqlen = tensor.shape[dim]\n",
        "    m = seqlen / multiple\n",
        "    if m.is_integer():\n",
        "        return False, tensor\n",
        "    remainder = math.ceil(m) * multiple - seqlen#calculating the remaninder after the multiple padding has happened\n",
        "    pad_offset = (0,) * (-1 - dim) * 2\n",
        "    return True, F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
        "\n",
        "def look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):\n",
        "    '''This is a function for padding our x with the sliding window attention mechanism first getting the shapes and dimensions\n",
        "    then padding the x from backward to forward which is of range of window 2n+1\n",
        "    now we iteratively pad with different combination of windows by loop over the forward+backward+1\n",
        "    and finally concatenating all the different tensors that are formed resulting in our final attention'''\n",
        "    t = x.shape[1]\n",
        "    dims = (len(x.shape) - dim) * (0, 0)\n",
        "    padded_x = F.pad(x, (*dims, backward, forward), value = pad_value)\n",
        "    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)]\n",
        "    return torch.cat(tensors, dim = dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12,window_size=3, n_ctx=1024, d_head=64, bias=True, scale=False,look_forward=1,look_backward=1):\n",
        "        '''An implementation of a sliding window attention, as proposed in Longformer I am also combing the rotationaol embeddings with it for\n",
        "        checking out the results\n",
        "        Params:\n",
        "        d_model:The dimension that needs to be feed into our model\n",
        "        n_head:The number of heads for attention\n",
        "        n_ctx:a parameters for buffer registry for bias\n",
        "        d_head:the dimension head output\n",
        "        bias:A bool for including or not including bias\n",
        "        scale: Whether to scale the attention scores by the square root of the dimension of the queries(use sqrt(dk) or not) \"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.n_head  = n_head\n",
        "        self.d_model = d_model\n",
        "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
        "        self.proj_out = nn.Linear(n_head * d_head, d_model)\n",
        "        self.scale   = scale\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.c_proj  = Conv1D(d_model, d_model)\n",
        "        # self.rotary_emb = RotaryEmbedding(dim = 32)#intializing the rotatory embedding with dimension 32\n",
        "        self.window_size = window_size\n",
        "        # Properties specific to this attention mechanism\n",
        "        self.supports_attention_mask = True\n",
        "        self.supports_key_padding_mask = False\n",
        "\n",
        "        self.attention_mask: Optional[torch.Tensor] = None#attention mask to store the values of the slided attention window\n",
        "        self.requires_same_k_q_dimensions = True\n",
        "\n",
        "        self.look_backward=look_backward\n",
        "        self.look_forward=look_forward\n",
        "\n",
        "        self.causal=False\n",
        "        self.force_sparsity=False\n",
        "        self.shared_qk=False\n",
        "\n",
        "        self.attn_mask=None\n",
        "        self.TOKEN_SELF_ATTN_VALUE = -5e4\n",
        "\n",
        "    def _get_local_mask(self, shape: torch.Size) -> AttentionMask:\n",
        "      self.window_size = min(self.window_size * 2 + 1, shape[1]) if self.causal else min(self.window_size, shape[1])\n",
        "      mask = local_1d_pattern(shape[1], window_size)\n",
        "\n",
        "      if self.causal:\n",
        "          mask &= causal_1d_pattern(shape[1])\n",
        "\n",
        "      mask = sparsify(mask) if self.force_sparsity else maybe_sparsify(mask)\n",
        "\n",
        "      # Convert mask to tensor and set its dtype to float32\n",
        "      mask_tensor = mask.to(torch.float32)\n",
        "\n",
        "      return AttentionMask(mask_tensor)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        spliting inyo given number of heads and then returning\n",
        "        return shape [`batch`, `head`, `sequence`, `features`]\n",
        "        \"\"\"\n",
        "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n",
        "        x = x.view(*new_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def _attn(self, q, k, v, attn_mask=None):\n",
        "        \"\"\"The main attention function\n",
        "        That claculates the attention using our dot product formula\"\"\"\n",
        "        scores  = torch.matmul(q, k.transpose(-2, -1))# dot multiplication between q and k transpose\n",
        "        if self.scale: scores = scores/math.sqrt(v.size(-1))# scaling it by dividing by sqrt(dk)\n",
        "        nd, ns  = scores.size(-2), scores.size(-1)\n",
        "        if attn_mask is not None: scores = scores + attn_mask# adding scores with attention mask values\n",
        "        scores  = self.softmax(scores)# adding softmax values\n",
        "        scores  = self.dropout(scores) #dropout of 0.1 as mentioned\n",
        "        outputs = torch.matmul(scores, v) # now the final matrix multiplication between score and V\n",
        "        return outputs\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        # merging the attention heads into one\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
        "        return x.view(*new_shape)\n",
        "\n",
        "\n",
        "    def forward(self, x,mask = None,input_mask = None,attn_bias = None,window_size = None):\n",
        "        '''The feed forward function that calculates the attention, split the heads, make attention, merge heads and project out the output\n",
        "        Applies convolutional attention to the input tensor.\n",
        "Splits the query, key, and value tensors into heads.\n",
        "Applies rotary embeddings to the query and key.\n",
        "Dynamically sets the window size if provided.\n",
        "Asserts that the sequence length is divisible by the window size.\n",
        "Applies the sliding window attention mechanism.\n",
        "Computes attention, applies masks, and performs aggregation.\n",
        "Returns the final output tensor.'''\n",
        "        mask = default(mask, input_mask)\n",
        "\n",
        "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
        "        q, k, v  = x.split(self.d_model, dim=2)\n",
        "\n",
        "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "        #applying the rotatory embeddings over query and key\n",
        "        # q = self.rotary_emb.rotate_queries_or_keys(q)\n",
        "        # k = self.rotary_emb.rotate_queries_or_keys(k)\n",
        "        shape, pad_value, window_size, causal, look_backward, look_forward, shared_qk = q.shape, -1, default(window_size, self.window_size), self.causal, self.look_backward, self.look_forward, self.shared_qk\n",
        "        (q, packed_shape), (k, _), (v, _) = map(lambda t: pack([t], '* n d'), (q, k, v))\n",
        "        b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype\n",
        "\n",
        "        scale = default(self.scale, dim_head ** -0.5)\n",
        "\n",
        "        if window_size is not None:\n",
        "          self.window_size = window_size  # Set the window size dynamically\n",
        "\n",
        "        assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n",
        "\n",
        "        windows = n // window_size\n",
        "\n",
        "\n",
        "        seq = torch.arange(n, device = device)\n",
        "        b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)\n",
        "\n",
        "        bq, bk, bv = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k, v))\n",
        "\n",
        "        bq = bq * scale\n",
        "\n",
        "        look_around_kwargs = dict(\n",
        "            backward =  look_backward,\n",
        "            forward =  look_forward,\n",
        "            pad_value = pad_value\n",
        "        )\n",
        "\n",
        "        bk = look_around(bk, **look_around_kwargs)\n",
        "        bv = look_around(bv, **look_around_kwargs)\n",
        "\n",
        "        bq_t = b_t\n",
        "        bq_k = look_around(b_t, **look_around_kwargs)\n",
        "\n",
        "        bq_t = rearrange(bq_t, '... i -> ... i 1')\n",
        "        bq_k = rearrange(bq_k, '... j -> ... 1 j')\n",
        "\n",
        "        pad_mask = bq_k == pad_value\n",
        "\n",
        "        sim = einsum('b h i e, b h j e -> b h i j', bq, bk)\n",
        "\n",
        "        if exists(attn_bias):\n",
        "            heads = attn_bias.shape[0]\n",
        "            assert (b % heads) == 0\n",
        "\n",
        "            attn_bias = repeat(attn_bias, 'h i j -> (b h) 1 i j', b = b // heads)\n",
        "            sim = sim + attn_bias\n",
        "\n",
        "        mask_value = max_neg_value(sim)\n",
        "\n",
        "        if shared_qk:\n",
        "            self_mask = bq_t == bq_k\n",
        "            sim = sim.masked_fill(self_mask, self.TOKEN_SELF_ATTN_VALUE)\n",
        "            del self_mask\n",
        "\n",
        "\n",
        "        sim = sim.masked_fill(pad_mask, mask_value)\n",
        "\n",
        "        # take care of key padding mask passed in\n",
        "\n",
        "        if exists(mask):\n",
        "            batch = mask.shape[0]\n",
        "            assert (b % batch) == 0\n",
        "\n",
        "            h = b // mask.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "            mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n",
        "            mask = look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n",
        "            mask = rearrange(mask, '... j -> ... 1 j')\n",
        "            mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n",
        "            sim = sim.masked_fill(~mask, mask_value)\n",
        "            del mask\n",
        "\n",
        "        # attention\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # aggregation\n",
        "\n",
        "        out = einsum('b h i j, b h j e -> b h i e', attn, bv)\n",
        "        out = rearrange(out, 'b w n d -> b (w n) d')\n",
        "\n",
        "        # out = self.proj_out(out)\n",
        "        out, *_ = unpack(out, packed_shape, '* n d')\n",
        "        out=rearrange(out, 'b n s d -> b s (n d)')\n",
        "        return out"
      ],
      "metadata": {
        "id": "Jeby2jdvAZ-_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=SlidingWindowAttention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
        "d_model = 768\n",
        "\n",
        "# Create a dummy variable\n",
        "dummy_out = torch.randn(36, 63, d_model)\n",
        "out=a(dummy_out)\n",
        "x=torch.randn(36, 63, 64)\n",
        "y=torch.randn(36, 63, 64)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhk6tTaeAcMD",
        "outputId": "81e713a2-5d62-4e70-df9b-c7a878e77c88"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([36, 63, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock_SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, dropout=0.1,window_size=2):\n",
        "        self.window_size=window_size\n",
        "        super().__init__()\n",
        "        self.attn        = SlidingWindowAttention(d_model=768,window_size=window_size, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
        "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
        "        self.ln_1        = LayerNorm(d_model)\n",
        "        self.ln_2        = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x),window_size=window_size)\n",
        "        x = x + self.feedforward(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "_EtXtPs9Alxj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_clones(module, n):\n",
        "    '''Here we can make certain copies of transformers'''\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(n)])"
      ],
      "metadata": {
        "id": "jdplS8itEJHf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size=5"
      ],
      "metadata": {
        "id": "BSerx4GcAn44"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2_SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
        "        super(GPT2_SlidingWindowAttention, self).__init__()\n",
        "        self.nlayers = nlayers\n",
        "        block        = TransformerBlock_SlidingWindowAttention(window_size=window_size,d_model=768, n_head=12, dropout=0.1)\n",
        "        self.h       = _get_clones(block, 12)\n",
        "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
        "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
        "        self.drop    = nn.Dropout(0.1)\n",
        "        self.ln_f    = LayerNorm(d_model)\n",
        "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.init_weights()\n",
        "\n",
        "    def set_window_size(self, window_size):\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def init_weights(self):\n",
        "        '''Initialization of weights'''\n",
        "        self.out.weight = self.wte.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        '''If the Linear, Embedding and Conv1D then nomrally initializing with mean and S.D'''\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "                '''Data Bias zero'''\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, src, labels=None, pos_ids=None):\n",
        "        '''Adding the positional embedding, dropping, then adding inputs, logits and outputs which are being used for loss function and then adding outputs and loss'''\n",
        "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
        "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
        "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
        "        inp     = self.ln_f(inp)\n",
        "        logits  = self.out(inp)\n",
        "        outputs = (logits,) + (inp,)\n",
        "\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "            return outputs\n",
        "        return logits"
      ],
      "metadata": {
        "id": "rZtuDB6_Apsx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained_weights from hugging face\n",
        "# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.`\n",
        "\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKF8Y1SmEPhN",
        "outputId": "286b2c94-7f57-485a-f8aa-469d60b6927a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-17 15:27:26--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.106.200, 52.217.175.120, 52.217.171.48, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.106.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548118077 (523M) [application/octet-stream]\n",
            "Saving to: ‘gpt2-pytorch_model.bin’\n",
            "\n",
            "gpt2-pytorch_model. 100%[===================>] 522.73M  13.3MB/s    in 41s     \n",
            "\n",
            "2023-12-17 15:28:09 (12.7 MB/s) - ‘gpt2-pytorch_model.bin’ saved [548118077/548118077]\n",
            "\n",
            "--2023-12-17 15:28:09--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2023-12-17 15:28:09--\n",
            "Total wall clock time: 42s\n",
            "Downloaded: 1 files, 523M in 41s (12.7 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2_SlidingWindowAttention()\n",
        "# load pretrained_weights from hugging face\n",
        "\n",
        "model_dict = model.state_dict() #currently with random initialization\n",
        "state_dict = torch.load(\"/content/gpt2-pytorch_model.bin\") #pretrained weights"
      ],
      "metadata": {
        "id": "fegFoJJeAr2_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_keys = []\n",
        "new_keys = []\n",
        "for key in state_dict.keys():\n",
        "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
        "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
        "        new_keys.append(new_key)\n",
        "        old_keys.append(key)\n",
        "\n",
        "for old_key, new_key in zip(old_keys, new_keys):\n",
        "    state_dict[new_key]=state_dict.pop(old_key)\n",
        "\n",
        "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval() #model in inference mode as it's now initialized with pretrained weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhhkfhMfAtkp",
        "outputId": "a26b4bb3-4085-4bc7-bebe-ba86eb96b338"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2_SlidingWindowAttention(\n",
              "  (h): ModuleList(\n",
              "    (0-11): 12 x TransformerBlock_SlidingWindowAttention(\n",
              "      (attn): SlidingWindowAttention(\n",
              "        (c_attn): Conv1D()\n",
              "        (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm()\n",
              "      (ln_2): LayerNorm()\n",
              "    )\n",
              "  )\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (ln_f): LayerNorm()\n",
              "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  (loss_fn): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Calculate size in bytes and megabytes\n",
        "size_bytes = total_params * 4  # Assuming float32 parameters, where each parameter is 4 bytes\n",
        "size_mb = size_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"Total size of the GPT-2 with rotatory embeddings and sliding window attention is: {size_bytes} bytes or {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Dx3NXDAvrh",
        "outputId": "adb5867e-1b6c-4baa-a7e3-136c33dd1042"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the GPT-2 with rotatory embeddings and sliding window attention is: 526107648 bytes or 501.74 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "context = torch.tensor([tokenizer.encode(\"Hi Contlo, How\")])\n",
        "\n",
        "def generate_dynamic(context, window_size, ntok=20):\n",
        "    start_time = time.time()\n",
        "    for _ in range(ntok):\n",
        "        model.set_window_size(window_size)  # Set the window size dynamically\n",
        "        out = model(context)\n",
        "        logits = out[:, -1, :]\n",
        "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = np.NINF\n",
        "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
        "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # Dynamically adjust the length of the input sequence based on the window_size\n",
        "        input_length = context.size(-1)\n",
        "        padding_size = window_size - (input_length % window_size)\n",
        "        if padding_size != window_size:\n",
        "            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "            padding_tokens = torch.zeros((context.size(0), padding_size), dtype=torch.long, device=context.device) + pad_token_id\n",
        "            context = torch.cat([context, padding_tokens], dim=-1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    return context, inference_time\n",
        "\n",
        "# Usage\n",
        "window_size = 5  # Adjust this as needed\n",
        "out, inference_time = generate_dynamic(context, ntok=20, window_size=window_size)\n",
        "decoded_output = tokenizer.decode(out[0])\n",
        "\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "print(f\"Generated Output: {decoded_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjIzD-uYCven",
        "outputId": "8247e83b-7373-4c20-d267-836b4bf9ae39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time: 7.4760 seconds\n",
            "Generated Output: Hi Contlo, How you!!!!\n",
            "!!!! and!!!! a!!!!\n",
            "!!!! (!!!! and!!!!\n",
            "!!!!.!!!!,!!!! I!!!! (!!!! the!!!! the!!!! (!!!! a!!!!,!!!! the!!!! I!!!!.!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78SAIio6ZMxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}